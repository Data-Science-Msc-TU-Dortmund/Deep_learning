{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c22839d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning - Lab Session 4\n",
    "\n",
    "**Winter Semester 22/23**\n",
    "\n",
    "Welcome to the fourth lab. In this lab, we will derive the backpropagation equations, \n",
    "code the training procedure, and test it on our beloved dataset with five points.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb73323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T22:50:58.757305Z",
     "iopub.status.busy": "2022-11-02T22:50:58.756418Z",
     "iopub.status.idle": "2022-11-02T22:51:00.098587Z",
     "shell.execute_reply": "2022-11-02T22:51:00.098138Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from torch import Tensor\n",
    "\n",
    "set_matplotlib_formats('png', 'pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c7042",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 1\n",
    "Consider a neural network with $L$ layers and a loss function\n",
    "$\\mathcal{L}(\\textbf{y},\\textbf{z}^{(L)}_{\\cdot,out})$. Call the output of the $i$-th\n",
    "unit of the $\\ell$-th layer $\\textbf{z}^{(\\ell)}_{i,out}=\\sigma^{(\\ell)}(\\textbf{z}^{(\\ell)}_{i,in})$\n",
    "with $\\textbf{z}^{(\\ell)}_{i,in}=\\sum_j\\textbf{W}^{(\\ell)}_{ji}\\textbf{z}^{(\\ell-1)}_{j,out}+\\textbf{b}^{(\\ell)}_{i}$\n",
    "its pre-activation output. Finally, consider $\\delta^{(\\ell)}_i=\\partial\\mathcal{L}(\\textbf{y},\\textbf{z}^{(L)}_{\\cdot,out})/\\partial\\mathbf{z}^{(\\ell)}_{i,in}$\n",
    "the gradient of the loss with respect to the pre-activation outputs of layer $\\ell$.\n",
    "\n",
    "Derive the back-propagation algorithm for a network with arbitrary architecture.\n",
    "You might find the results of the previous lab a useful reference, as well as chapter\n",
    "5 of the book _Mathematics for Machine Learning_ (https://mml-book.github.io).\n",
    "\n",
    "\n",
    "1. Show that\n",
    "\n",
    "\\begin{align}\n",
    "\\delta^{(L)}_i &= \\frac{\\partial\\mathcal{L}(\\textbf{y},\\textbf{z}^{(L)}_{\\cdot,out})}{\\partial\\textbf{z}^{(L)}_{i,out}}\n",
    "\\cdot{\\sigma^\\prime}^{(L)}(\\textbf{z}^{(L)}_{i,in})\n",
    "\\\\\n",
    "\\frac{\\partial \\mathcal{L}(\\textbf{y},\\textbf{z}^{(L)}_{\\cdot,out})}{\\partial \\textbf{W}^{(\\ell)}_{ji}}&=\\delta^{(\\ell)}_i\\cdot\\textbf{z}^{(\\ell-1)}_{j,out} \\\\\n",
    "\\frac{\\partial \\mathcal{L}(\\textbf{y},\\textbf{z}^{(L)}_{\\cdot,out})}{\\partial \\textbf{b}^{(\\ell)}_{i}}&=\\delta^{(\\ell)}_i \\\\\n",
    "\\delta^{(\\ell-1)}_i&=\\left(\\sum_k\\delta^{(\\ell)}_k\\cdot\\textbf{W}^{(\\ell)}_{ik}\\right)\\cdot{\\sigma^\\prime}^{(\\ell-1)}(\\textbf{z}^{(\\ell-1)}_{i,in})\n",
    "\\end{align}\n",
    "\n",
    "2. Use vectorized operations (i.e., operations with vectors and matrices) to compute\n",
    "the gradients with respect to a single sample.\n",
    "\n",
    "3. Extend the vectorized operations to handle data in batches, and show that:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta^{(L)}&=\\nabla_{\\textbf{Z}^{(L)}_{out}}\\mathcal{L}(\\textbf{Y},\\textbf{Z}^{(L)}_{out})\\odot{\\sigma^\\prime}^{(L)}(\\textbf{Z}^{(L)}_{in}) \\\\\n",
    "\\nabla_{\\textbf{W}^{(\\ell)}}\\mathcal{L}(\\textbf{Y},\\textbf{Z}^{(\\ell)}_{out})&={\\textbf{Z}^{(\\ell-1)}_{out}}^T \\cdot\\Delta^{(\\ell)} \\\\\n",
    "\\nabla_{\\textbf{b}^{(\\ell)}}\\mathcal{L}(\\textbf{Y},\\textbf{Z}^{(L)}_{out})&=\\sum_i {\\Delta^{(\\ell)}_i}^T \\\\\n",
    "\\Delta^{(\\ell-1)}&=\\Delta^{(\\ell)}{\\textbf{W}^{(\\ell)}}^T\\odot{\\sigma^\\prime}^{(\\ell-1)}(\\textbf{Z}^{(\\ell-1)}_{in})\n",
    "\\end{align}\n",
    "\n",
    "where $\\Delta^{(\\ell)}$, $\\textbf{Y}$ and $\\textbf{Z}^{(\\ell)}_{out}$ are matrices whose $i$-th row contain the respective vectors $\\delta$, $\\textbf{y}$ and $\\textbf{z}^{(\\ell)}_{\\cdot,out}$ for the $i$-th sample in the batch, and $\\odot$ is the element-wise product.\n",
    "\\end{enumerate}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c618d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 2\n",
    "In this exercise, we will code the backpropagation algorithm\n",
    "and apply it to our five-points dataset.\n",
    "\n",
    "First, let's define some structures to quickly create a neural network with layers of\n",
    "given size. It will use tanh activation in the hidden layers and sigmoid for\n",
    "the output layer. Although we will use it for classification, we use the mean squared\n",
    "error loss for a change.\n",
    "\n",
    "**NOTE**: We use PyTorch only as computation engine. To showcase how backpropagation\n",
    "works under the hood, we do not utilize auto diff or other structures like modules or\n",
    "autograd functions in this example (just basic OOP). However, we still use some\n",
    "conventions like forward/backward notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55c5383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T22:51:00.101012Z",
     "iopub.status.busy": "2022-11-02T22:51:00.100816Z",
     "iopub.status.idle": "2022-11-02T22:51:00.107361Z",
     "shell.execute_reply": "2022-11-02T22:51:00.107008Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        self.weight = self._init_glorot(in_features, out_features)\n",
    "        self.bias = torch.zeros(out_features)\n",
    "\n",
    "        self.weight_grad: Optional[Tensor] = None\n",
    "        self.bias_grad: Optional[Tensor] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_glorot(in_features: int, out_features: int) -> Tensor:\n",
    "        \"\"\"Init a weight matrix with glorot initialization.\"\"\"\n",
    "        b = torch.sqrt(torch.tensor([6. / (in_features + out_features)]))\n",
    "        return (2 * b) * torch.rand(in_features, out_features) - b\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.func = lambda x: 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.func(x)\n",
    "\n",
    "    def get_gradient(self, x: Tensor) -> Tensor:\n",
    "        return self.func(x) * (1 - self.func(x))\n",
    "\n",
    "\n",
    "class TanH:\n",
    "    @staticmethod\n",
    "    def forward(x: Tensor) -> Tensor:\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gradient(x: Tensor) -> Tensor:\n",
    "        return  1 - torch.tanh(x)**2\n",
    "\n",
    "class MSELoss:\n",
    "    @staticmethod\n",
    "    def forward(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "        return torch.mean((y_true - y_pred)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gradient(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "        return 2 * (y_pred - y_true) / len(y_true)\n",
    "\n",
    "\n",
    "# Now we bring everything together and create our neural network.\n",
    "class NeuralLayer:\n",
    "    def __init__(self, in_features: int, out_features: int, activation: str):\n",
    "        self.linear = Linear(in_features, out_features)\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            self.act = Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.act = TanH()\n",
    "        else:\n",
    "            raise ValueError('{} activation is unknown'.format(activation))\n",
    "\n",
    "        # We save the last computation as we'll need it for the backward pass.\n",
    "        self.last_input: Optional[None] = None\n",
    "        self.last_zin: Optional[None] = None\n",
    "        self.last_zout: Optional[None] = None\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        self.last_input = x\n",
    "        self.last_zin = self.linear.forward(x)\n",
    "        self.last_zout = self.act.forward(self.last_zin)\n",
    "        return self.last_zout\n",
    "\n",
    "    def get_weight(self) -> Tensor:\n",
    "        \"\"\"Get the weight matrix in the linear layer.\"\"\"\n",
    "        return self.linear.weight\n",
    "\n",
    "    def get_bias(self) -> Tensor:\n",
    "        \"\"\"Get the weight matrix in the linear layer.\"\"\"\n",
    "        return self.linear.bias\n",
    "\n",
    "    def set_weight_gradient(self, grad: Tensor) -> None:\n",
    "        \"\"\"Set a tensor as gradient for the weight in the linear layer.\"\"\"\n",
    "        self.linear.weight_grad = grad\n",
    "\n",
    "    def set_bias_gradient(self, grad: Tensor) -> None:\n",
    "        \"\"\"Set a tensor as gradient for the bias in the linear layer.\"\"\"\n",
    "        self.linear.bias_grad = grad\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes: List[int]):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        self.layers: List[NeuralLayer] = []\n",
    "        layer_sizes = [self.input_size] + self.hidden_sizes\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.layers.append(NeuralLayer(layer_sizes[i-1], layer_sizes[i], 'tanh'))\n",
    "        self.layers.append(NeuralLayer(hidden_sizes[-1], self.output_size, 'sigmoid'))\n",
    "\n",
    "        self.loss = MSELoss()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "# TODO perform the forward pass and return the predictions.\n",
    "\n",
    "    def get_loss(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        \"\"\"Compute the loss for a dataset and given labels.\"\"\"\n",
    "# TODO: Use the loss function and the forward method to compute the loss on the dataset.\n",
    "\n",
    "    def backward(self, x: Tensor, y: Tensor) -> None:\n",
    "        \"\"\"Compute all gradients over backpropagation.\"\"\"\n",
    "        # Perform forward pass.\n",
    "        # The z's are automatically saved by our NeuralLayer object.\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "# TODO: Compute the gradients.\n",
    "# Hint: Rely on the objects and structures we defined above. (Especially NeuralLayer!)\n",
    "# Also remember that the `z_in` and `z_out`s are saved in the the `linear` object of NeuralLayer.\n",
    "\n",
    "        # Check if gradients have the right size.\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if layer.linear.weight_grad.shape != layer.linear.weight.shape \\\n",
    "                or layer.linear.bias_grad.shape != layer.linear.bias.shape:\n",
    "                raise ValueError('Gradients in layer with index {} have a wrong shape.'\n",
    "                                 .format(i))\n",
    "\n",
    "\n",
    "    def apply_gradients(self, learning_rate: float) -> None:\n",
    "        \"\"\"Update weights with the computed gradients.\"\"\"\n",
    "# TODO: Apply the gradients that are stashed in NeuralLayer/Linear to perform gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0e426",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After we have defined our network, we can create it and test if the passes work without\n",
    "errors on our small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9103554b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T22:51:00.109209Z",
     "iopub.status.busy": "2022-11-02T22:51:00.109031Z",
     "iopub.status.idle": "2022-11-02T22:51:00.148768Z",
     "shell.execute_reply": "2022-11-02T22:51:00.145996Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, -1],\n",
    "    [-1, 0],\n",
    "    [0, 1]\n",
    "], dtype=torch.float)\n",
    "y = torch.tensor([1, 0, 0, 0, 0])\n",
    "\n",
    "network = NeuralNetwork(\n",
    "    input_size=2,\n",
    "    hidden_sizes=[5, 3],\n",
    "    output_size=1\n",
    ")\n",
    "\n",
    "print(network.forward(x))\n",
    "network.backward(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c9a94",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can inspect the decision boundary as in the\n",
    "previous exercises for the randomly initialized network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ccdd25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T22:51:00.179275Z",
     "iopub.status.busy": "2022-11-02T22:51:00.177540Z",
     "iopub.status.idle": "2022-11-02T22:51:00.199488Z",
     "shell.execute_reply": "2022-11-02T22:51:00.196927Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_decision_boundary(x: Tensor, y: Tensor, net: NeuralNetwork) -> None:\n",
    "    grid_range = torch.linspace(-2, 2, 50)\n",
    "    grid_x, grid_y = torch.meshgrid(grid_range, grid_range)\n",
    "    grid_data = torch.stack([grid_x.flatten(), grid_y.flatten()]).T\n",
    "\n",
    "    predictions = net.forward(grid_data)\n",
    "\n",
    "    plt.contourf(grid_x, grid_y, predictions.view(grid_x.shape))\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(x, y, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff3f56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now finally train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b67a656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T22:51:00.210043Z",
     "iopub.status.busy": "2022-11-02T22:51:00.209181Z",
     "iopub.status.idle": "2022-11-02T22:51:00.230994Z",
     "shell.execute_reply": "2022-11-02T22:51:00.228661Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(\n",
    "        x: Tensor,\n",
    "        y: Tensor,\n",
    "        net: NeuralNetwork,\n",
    "        epochs: int,\n",
    "        lr: float\n",
    ") -> Tuple[NeuralNetwork, Tensor]:\n",
    "    \"\"\"\n",
    "    Train a neural network.\n",
    "    :param x: Training dataset.\n",
    "    :param y: Training labels.\n",
    "    :param net: Neural network to train.\n",
    "    :param epochs: Number of training epochs.\n",
    "    :param lr: Learning rate for gradient descent.\n",
    "    :return: Trained network and losses over course of training.\n",
    "    \"\"\"\n",
    "\n",
    "# TODO iterate over the dataset for the given number of epochs and modify the weights at each epoch.\n",
    "# Use all the data to compute the gradients.\n",
    "# Also calculate the loss, so that we can track how the training is going.\n",
    "    return net, torch.stack(losses)\n",
    "\n",
    "network, losses = train(x, y, network, 2500, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc0d03",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By plotting the loss after each parameter update,\n",
    "we can be sure that the network converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc41c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T22:51:00.240939Z",
     "iopub.status.busy": "2022-11-02T22:51:00.240109Z",
     "iopub.status.idle": "2022-11-02T22:51:00.262331Z",
     "shell.execute_reply": "2022-11-02T22:51:00.259350Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9b9f4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And the decision boundary of the network is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f99a4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T22:51:00.269489Z",
     "iopub.status.busy": "2022-11-02T22:51:00.269088Z",
     "iopub.status.idle": "2022-11-02T22:51:00.279420Z",
     "shell.execute_reply": "2022-11-02T22:51:00.278406Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_decision_boundary(x, y, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932bb330",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try to train a few randomly initialized networks and vary depth and hidden sizes\n",
    "to discover different decision boundaries.\n",
    "Try to modify the learning rate and see how it affects the convergence speed.\n",
    "Finally, try different ways to initialize the weights and note how the\n",
    "trainability of the network is affected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
