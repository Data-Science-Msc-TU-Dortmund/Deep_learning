{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d298b30",
   "metadata": {},
   "source": [
    "# Deep Learning - Lab Session 3\n",
    "\n",
    "**Winter Semester 22/23**\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583b7faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.094466Z",
     "iopub.status.busy": "2022-10-25T13:53:04.093569Z",
     "iopub.status.idle": "2022-10-25T13:53:04.831544Z",
     "shell.execute_reply": "2022-10-25T13:53:04.831019Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66f719",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Welcome to the third lab. The first exercise is an implementation of gradient descent\n",
    "on a bivariate function. The second exercise is about computing derivatives of the\n",
    "weights of a neural network, and the third exercise combines the previous two.\n",
    "\n",
    "## Exercise 1\n",
    "This exercise is about gradient descent. We will use the function\n",
    "$f(x_1, x_2)=(x_1-6)^2+x_2^2-x_1x_2$ as a running example:\n",
    "\n",
    " 1. Use pen and paper to do three iterations of gradient descent:\n",
    "     - Find the gradient of $f$;\n",
    "     - Start from the point $x_1=x_2=6$ and use a step size of $1/2$ for the first step,\n",
    "    $1/3$ for the second step and $1/4$ for the third step;\n",
    "     - What will happen if you keep going?\n",
    " 2. Write a function that performs gradient descent:\n",
    "     - For simplicity, we use a constant learning rate.\n",
    "     - Can you find a way to prematurely stop the optimization when you are close to the\n",
    "    optimum?\n",
    "     -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188eb4db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.833759Z",
     "iopub.status.busy": "2022-10-25T13:53:04.833569Z",
     "iopub.status.idle": "2022-10-25T13:53:04.838830Z",
     "shell.execute_reply": "2022-10-25T13:53:04.838419Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: Defining a custom autograd function is not a necessity for this small task,\n",
    "# but it is a good place to showcase some capabilities of PyTorch.\n",
    "\n",
    "class MyFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, x: Tensor) -> Tensor:\n",
    "        # The \"ctx\" object serves to stash information for the backward pass\n",
    "        ctx.save_for_backward(x)\n",
    "        func_value = (\n",
    "# TODO compute the value of f at x.\n",
    "        )\n",
    "        return func_value\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_output: Tensor):\n",
    "        # The \"grad_output\" parameter is the backpropagated gradient from subsequent\n",
    "        # operations w.r.t. to the output of this function.\n",
    "        x = ctx.saved_tensors[0]\n",
    "\n",
    "        grad_x = torch.tensor([\n",
    "# TODO compute the gradient of f at x.\n",
    "        ])\n",
    "        return grad_output * grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2d75ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.840590Z",
     "iopub.status.busy": "2022-10-25T13:53:04.840440Z",
     "iopub.status.idle": "2022-10-25T13:53:04.870776Z",
     "shell.execute_reply": "2022-10-25T13:53:04.869080Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "func = MyFunction()\n",
    "# The \"required_grad\" argument needs to be True.\n",
    "# Otherwise no gradients will be computed.\n",
    "x = torch.tensor([6., 6.], requires_grad=True)\n",
    "\n",
    "# Custom functions are applied over the \"apply\" method.\n",
    "y = func.apply(x)\n",
    "print('Function output: {}'.format(y))\n",
    "\n",
    "# Gradients for every operation in this chain are computed\n",
    "# by calling the \"backward\" method on the output tensor.\n",
    "y.backward()\n",
    "\n",
    "# The x tensor now has a grad attribute with the gradients.\n",
    "print('Gradients: {}'.format(x.grad))\n",
    "\n",
    "# Note: No usage of auto differentiation was done in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6d677",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Does it match what you computed?\n",
    "\n",
    "In the next step we define a small gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7c74f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.877961Z",
     "iopub.status.busy": "2022-10-25T13:53:04.877386Z",
     "iopub.status.idle": "2022-10-25T13:53:04.893112Z",
     "shell.execute_reply": "2022-10-25T13:53:04.891430Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class GradientDescentOptimizer:\n",
    "    def __init__(self,\n",
    "                 func: Function,\n",
    "                 max_steps: int,\n",
    "                 alpha: float):\n",
    "        \"\"\"\n",
    "        Init an Optimizer for performing GD.\n",
    "\n",
    "        :param func: Function to apply.\n",
    "        :param max_steps: Maximum number of GD steps.\n",
    "        :param alpha: Learning Rate.\n",
    "        \"\"\"\n",
    "        self.func = func\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply GD on a tensor.\n",
    "\n",
    "        :param x: Input tensor.\n",
    "        \"\"\"\n",
    "        # Usually you would apply the gradients inplace on the input tensor,\n",
    "        # but for the sake of the example we keep the input tensor consistent and\n",
    "        # work on a copy.\n",
    "        x_cp = x.detach().clone()\n",
    "        x_cp.requires_grad = True\n",
    "\n",
    "# TODO use a for loop to do gradient descent.\n",
    "# HINT When applying gradients you will need an \"torch.no_grad()\" context\n",
    "# manager. To modify the content of the tensor you will need its \".data\"\n",
    "# attribute. Don't forget to erase the gradients after each iteration or\n",
    "# or they will accumulate.\n",
    "        return x_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15604541",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.899934Z",
     "iopub.status.busy": "2022-10-25T13:53:04.899367Z",
     "iopub.status.idle": "2022-10-25T13:53:04.913298Z",
     "shell.execute_reply": "2022-10-25T13:53:04.911724Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([6., 6.], requires_grad=True)\n",
    "gd_optimizer = GradientDescentOptimizer(func=MyFunction(), max_steps=10, alpha=0.1)\n",
    "x_new = gd_optimizer(x)\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc821c6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Play a bit with the starting point and learning rate to get a feel for its behavior.\n",
    "How close can you get to the minimum?\n",
    "\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "This exercise is about computing gradients with the chain rule, with pen and paper.\n",
    "We will work with a neural network with a single hidden layer with two neurons and an\n",
    "output layer with one neuron.\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "![Neural network used in Exercise 2](../utils/03-lab-nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfec570",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The neurons in the hidden layer use the $\\tanh$ activation, while the output neuron uses\n",
    "the sigmoid. The loss used in binary classification is the _binary cross-entropy_:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y, f_{out})=-y\\log f_{out}-(1-y)\\log(1-f_{out})\n",
    "$$\n",
    "\n",
    "where $y\\in\\{0,1\\}$ is the true label and $f_{out}\\in(0,1)$ is the predicted probability that $y=1$.\n",
    "\n",
    " 1. Compute $\\partial\\mathcal{L}(y, f_{out})/\\partial f_{out}$\n",
    " 2. Compute $\\partial f_{out}/\\partial f_{in}$\n",
    " 3. Show that $\\partial\\sigma(x)/\\partial x=\\sigma(x)(1-\\sigma(x))$\n",
    " 4. Show that $\\partial\\tanh(x)/\\partial x=1-\\tanh(x)^2$ (Hint: $\\tanh(x)=(e^x-e^{-x})(e^x+e^{-x})^{-1}$)\n",
    " 5. Compute $\\partial f_{in}/\\partial c$\n",
    " 6. Compute $\\partial f_{in}/\\partial u_1$\n",
    " 7. Compute $\\partial\\mathcal{L}(y,  f_{out})/\\partial c$\n",
    " 8. Compute $\\partial\\mathcal{L}(y,  f_{out})/\\partial u_1$\n",
    " 9. Compute $\\partial f_{in}/\\partial z_{2,out}$\n",
    " 10. Compute $\\partial z_{2,out}/\\partial z_{2,in}$\n",
    " 11. Compute $\\partial z_{2,in}/\\partial b_2$\n",
    " 12. Compute $\\partial z_{2,in}/\\partial w_{12}$\n",
    " 13. Compute $\\partial z_{2,in}/\\partial x_1$\n",
    " 14. Compute $\\partial\\mathcal{L}(y,  f_{out})/\\partial b_2$\n",
    " 15. Compute $\\partial\\mathcal{L}(y,  f_{out})/\\partial w_{12}$\n",
    " 16. Compute $\\partial\\mathcal{L}(y,  f_{out})/\\partial x_1$\n",
    "\n",
    "You will notice that there are lots of redundancies. We will see how to improve these\n",
    "computations in the lecture and in the next lab. Luckily, modern deep learning software\n",
    " computes gradients automatically for you.\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Now that we know how to do gradient descent and how to compute the derivatives of the\n",
    "weights of a simple network, we can try to do these steps together and train our first\n",
    "neural network! We will use the small dataset with five points we studied in the first\n",
    "lab.\n",
    "\n",
    "First, let's define the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4602a5a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.920491Z",
     "iopub.status.busy": "2022-10-25T13:53:04.919937Z",
     "iopub.status.idle": "2022-10-25T13:53:04.929135Z",
     "shell.execute_reply": "2022-10-25T13:53:04.927362Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, -1],\n",
    "    [-1, 0],\n",
    "    [0, 1]\n",
    "], dtype=torch.float)\n",
    "y = torch.tensor([1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a576b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, a function to compute the output of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4287f3d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.936264Z",
     "iopub.status.busy": "2022-10-25T13:53:04.935717Z",
     "iopub.status.idle": "2022-10-25T13:53:04.949913Z",
     "shell.execute_reply": "2022-10-25T13:53:04.948537Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sigmoid(x: Tensor) -> Tensor:\n",
    "# TODO compute the sigmoid on x and return.\n",
    "\n",
    "def predict(x: Tensor, b1: float, b2: float,\n",
    "            w11: float, w12: float, w21: float, w22: float,\n",
    "            c: float, u1: float, u2:float) -> Tensor:\n",
    "# TODO compute and return the output of the network.\n",
    "\n",
    "# This should return the predictions for the five points in the datasets\n",
    "# We can unpack the param vector for the positional params of the function so that we don't\n",
    "# need to enter every single entry.\n",
    "params = torch.randn(9)\n",
    "predictions = predict(x, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0813612",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since gradient descent is done on the loss function, we need a function to compute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "578e0fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:04.957614Z",
     "iopub.status.busy": "2022-10-25T13:53:04.956240Z",
     "iopub.status.idle": "2022-10-25T13:53:04.969795Z",
     "shell.execute_reply": "2022-10-25T13:53:04.968362Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_loss(target: Tensor, pred: Tensor) -> Tensor:\n",
    "# TODO return the average loss.\n",
    "\n",
    "loss = get_loss(y, predictions)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47036ce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we need to compute the gradient of each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbaf0cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:05.018361Z",
     "iopub.status.busy": "2022-10-25T13:53:05.017030Z",
     "iopub.status.idle": "2022-10-25T13:53:05.036920Z",
     "shell.execute_reply": "2022-10-25T13:53:05.034790Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_gradients(x: Tensor, target: Tensor,\n",
    "            b1: float, b2: float,\n",
    "            w11: float, w12: float, w21: float, w22: float,\n",
    "            c: float, u1: float, u2:float) -> Tensor:\n",
    "    # First, we perform the forward pass.\n",
    "    z1in = b1 + x[:, 0] * w11 + x[:, 1] * w21\n",
    "    z1out = torch.tanh(z1in)\n",
    "\n",
    "    z2in = b2 + x[:, 0] * w12 + x[:, 1] * w22\n",
    "    z2out = torch.tanh(z2in)\n",
    "\n",
    "    fin = c + u1 * z1out + u2 * z2out\n",
    "    fout = sigmoid(fin)\n",
    "\n",
    "# TODO compute all the partial derivatives.\n",
    "    \n",
    "    # Return the derivatives in the same order as the parameters vector\n",
    "    return torch.stack([\n",
    "        dL_db1, dL_db2, dL_dw11, dL_dw12, dL_dw21, dL_dw22, dL_dc, dL_du1, dL_du2  \n",
    "    ])\n",
    "\n",
    "print(get_gradients(x, y, *params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f15728",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finite differences are a useful way to check that the gradients are computed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894524a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:05.046207Z",
     "iopub.status.busy": "2022-10-25T13:53:05.045500Z",
     "iopub.status.idle": "2022-10-25T13:53:05.065384Z",
     "shell.execute_reply": "2022-10-25T13:53:05.062900Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First, compute the analytical gradient of the parameters.\n",
    "gradient = get_gradients(x, y, *params)\n",
    "eps = 1e-9\n",
    "for i in range(9):\n",
    "    # Compute loss when subtracting eps to parameter i.\n",
    "    neg_params = params.clone()\n",
    "    neg_params[i] = neg_params[i] - eps\n",
    "    neg_value = get_loss(y, predict(x, *neg_params))\n",
    "\n",
    "    # Compute loss when adding eps to parameter i.\n",
    "    pos_params = params.clone()\n",
    "    pos_params[i] = pos_params[i] + eps\n",
    "    pos_value = get_loss(y, predict(x, *pos_params))\n",
    "\n",
    "    # Compute the \"empirical\" gradient of parameter i\n",
    "    fdiff_gradient = torch.mean((pos_value - neg_value) / (2 * eps))\n",
    "\n",
    "    # Error if difference is too large\n",
    "    if torch.abs(gradient[i] - fdiff_gradient) < 1e-5:\n",
    "        raise ValueError('Gradients are probably wrong!')\n",
    "\n",
    "print(\"Gradients are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915db9cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can finally train our network. Since the network is so small compared to the dataset,\n",
    " the training procedure is very sensitive to the way the weights are initialized and\n",
    " the step size used in gradient descent.\n",
    "\n",
    "Try to play around with the learning rate and the random initialization of the weights\n",
    "and find reliable values that make training successful in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc381f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:05.073736Z",
     "iopub.status.busy": "2022-10-25T13:53:05.073013Z",
     "iopub.status.idle": "2022-10-25T13:53:05.082959Z",
     "shell.execute_reply": "2022-10-25T13:53:05.082248Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_loss = 10\n",
    "alpha = 1.\n",
    "steps = 100\n",
    "best_params = None\n",
    "\n",
    "for i in range(10):\n",
    "    params = torch.randn(9)\n",
    "\n",
    "    # Do GD\n",
    "    for _ in range(steps):\n",
    "        gradients = get_gradients(x, y, *params)\n",
    "        params -= alpha * gradients\n",
    "\n",
    "    final_loss = get_loss(y, predict(x, *params))\n",
    "    print('RUN {} \\t LOSS {:.4f}'.format(i + 1, float(final_loss)))\n",
    "\n",
    "    if final_loss < min_loss:\n",
    "        best_params = params\n",
    "        min_loss = final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c62cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use the function in the previous lab to visualize the decision boundary of\n",
    "the best network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5501a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:05.086497Z",
     "iopub.status.busy": "2022-10-25T13:53:05.086253Z",
     "iopub.status.idle": "2022-10-25T13:53:05.092554Z",
     "shell.execute_reply": "2022-10-25T13:53:05.092044Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_decision_boundary(\n",
    "        x: Tensor, y: Tensor, grid_x: Tensor, grid_y, pred: Tensor) -> None:\n",
    "    \"\"\"Plot the estimated decision boundary for a 2D grid with predictions.\"\"\"\n",
    "    plt.contourf(grid_x, grid_y, pred.view(grid_x.shape))\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap='jet')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8e31a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:05.095226Z",
     "iopub.status.busy": "2022-10-25T13:53:05.095005Z",
     "iopub.status.idle": "2022-10-25T13:53:05.100127Z",
     "shell.execute_reply": "2022-10-25T13:53:05.099680Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_range = torch.linspace(-2, 2, 50)\n",
    "grid_x, grid_y = torch.meshgrid(grid_range, grid_range)\n",
    "grid_data = torch.stack([grid_x.flatten(), grid_y.flatten()]).T\n",
    "pred = predict(grid_data, *best_params)\n",
    "\n",
    "plot_decision_boundary(x, y, grid_x, grid_y, pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0f71e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Also try to visualize the decision boundary of network with random parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf914fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-25T13:53:05.102287Z",
     "iopub.status.busy": "2022-10-25T13:53:05.102122Z",
     "iopub.status.idle": "2022-10-25T13:53:05.106636Z",
     "shell.execute_reply": "2022-10-25T13:53:05.106210Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred = predict(grid_data, *torch.randn(9))\n",
    "plot_decision_boundary(x, y, grid_x, grid_y, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
